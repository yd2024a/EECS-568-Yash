{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f043f84",
   "metadata": {},
   "source": [
    "# University of Michigan, NA/EECS 568, ROB 530 \n",
    "# Mobile Robotics: Methods & Algorithms, Winter 2024\n",
    "\n",
    "## Homework 2 $-$ Nonlinear Filtering & InEKF\n",
    "\n",
    "- **See the course Canvas for syllabus, due dates, and homework and grading policies.**\n",
    "- This course utilizes an autograder to evaluate homework. For each of the five problems, carefully read the submission instructions and complete the TODOs in the corresponding Python script. Once you have finished coding, upload all scripts to GradeScope for automatic assessment and grading. Please note, the autograder retains only the latest score, not the highest.\n",
    "- For each problem, the autograder will check the **return value** of the function you implement. Additionally, the variables are printed in this notebook for you to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block to enable autoreload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec114c7c",
   "metadata": {},
   "source": [
    "### Problem 1. Nonlinear Filtering (60 points)\n",
    "\n",
    "#### Problem Setup\n",
    "We are given a setup, shown below, to estimate the 3D position of an object's center. The object is stationary, and there are also two fixed monocular cameras in the room. A previously developed object detection algorithm can provide the object's center in the 2D image coordinates. However, the depth is not observable by a single RGB image. Therefore, our goal is to combine measurements from two cameras to estimate the object's 3D position in the frame of camera 1. The relative pose (orientation and translation) between the two cameras is accurately known and given. \n",
    "\n",
    "![setup](problem1_setup.png)\n",
    "\n",
    "One of your colleagues (who is good at math and modeling!) has already formulated the problem. But he has very poor knowledge of state estimation, which is why he reached out to you for the rest of the solution. Luckily, you are taking the Mobile Robotics class, and you can help him out this time, so he doesn't lose his job! So far, we have the following.\n",
    "\n",
    "\n",
    "#### Measurement Models\n",
    "From elementary computer vision knowledge, we know that a monocular pinhole camera model takes the following form\n",
    "\\begin{equation*}\n",
    "    \\begin{bmatrix}\n",
    "    u \\\\ \n",
    "    v\n",
    "    \\end{bmatrix} = \\begin{bmatrix} \n",
    "    f_x & 0 \\\\\n",
    "    0 & f_y\n",
    "    \\end{bmatrix} \\cdot  \\frac{1}{p_z} \\cdot \\begin{bmatrix}\n",
    "    p_x \\\\ p_y\n",
    "    \\end{bmatrix} + \\begin{bmatrix}\n",
    "    c_x \\\\ \n",
    "    c_y\n",
    "    \\end{bmatrix} = K_f \\pi(p) + c,\n",
    "\\end{equation*}\n",
    "where $K_f = \\begin{bmatrix} \n",
    "    f_x & 0 \\\\\n",
    "    0 & f_y\n",
    "    \\end{bmatrix}$ is called intrinsic camera matrix, $\\begin{bmatrix}\n",
    "    c_x \\\\ \n",
    "    c_y\n",
    "    \\end{bmatrix}$ is the coordinates of the optical center of the image, and $\\pi(\\cdot)$ is the projection function (it takes a point and divides its coordinates by $z$, i.e., the last coordinate).\n",
    "\n",
    "We denote the object's center in frame 1 by ${^1p}$ and ${^2p}$ as the object's center in frame 2. Given the rotation matrix, $R$, and the translation vector, $t$ of frame 2 with respect to frame 1, we have the following relationship for the object's center between the two frames.\n",
    "$${^1p} = R  \\cdot {^2p} + t,$$\n",
    "and $${^2p} = R^\\top \\cdot {^1p} - R^\\top t .$$\n",
    "\n",
    "Combining everything so far, we have the following measurement models for camera 1\n",
    "\\begin{equation*}\n",
    "    z = \\begin{bmatrix}\n",
    "    u \\\\ \n",
    "    v\n",
    "    \\end{bmatrix} = {^1K_f} \\pi({^1p})+{^1c} := h_1({^1p}),\n",
    "\\end{equation*}\n",
    "and camera 2\n",
    "\\begin{equation*}\n",
    "    z = \\begin{bmatrix}\n",
    "    u \\\\ \n",
    "    v\n",
    "    \\end{bmatrix} = {^2K_f} \\pi({^2p})+{^2c} = {^2K_f} \\pi(R^\\top  \\cdot {^1p} - R^\\top t)+{^2c} =: h_2({^1p}).\n",
    "\\end{equation*}\n",
    "We also assume each measurement model is corrupted by a zero-mean white Gaussian noise.\n",
    "\\begin{align*}\n",
    "    z_1 &= h_1({^1p}) + v_1, \\quad v_1 \\sim \\mathcal{N}(0,\\Sigma_{v_1}), \\\\\n",
    "    z_2 &= h_2({^1p}) + v_2, \\quad v_2 \\sim \\mathcal{N}(0,\\Sigma_{v_2}). \\\\\n",
    "\\end{align*}\n",
    "We may also stack two synchronized observations to form a stacked observation model as follows.\n",
    "\\begin{align*}\n",
    "z = \\begin{bmatrix}\n",
    "    z_1 \\\\ z_2 \n",
    "    \\end{bmatrix}_{4\\times 1} = \\begin{bmatrix}\n",
    "    h_1({^1p}) \\\\ h_2({^1p}) \n",
    "    \\end{bmatrix}_{4\\times 1} + \\begin{bmatrix} \n",
    "    v_1 \\\\ v_2 \n",
    "    \\end{bmatrix}_{4\\times 1} =: h({^1p}) + v,\n",
    "\\end{align*}\n",
    "where now $v \\sim \\mathcal{N}\\left( 0_{4\\times 1},\\mathrm{blkdiag}(\\Sigma_{v_1},\\Sigma_{v_2})\\right)$, and $\\mathrm{blkdiag}(\\cdot)$ forms a block diagonal matrix.\n",
    "\n",
    "\n",
    "#### Motion Model\n",
    "Furthermore, camera 1 is attached to a structure that can vibrate. The vibration is not so severe that we assume the camera moves, but to account for inaccuracies caused by the fixture vibration we use a discrete-time random walk process as its motion model.\n",
    "$${^1p}_{k+1} = {^1p}_k + w, \\quad w \\sim \\mathcal{N}(0,\\Sigma_{w}).$$\n",
    "\n",
    "#### Jacobians\n",
    "The Jacobians are also given using the chain rule and the fact that if $q = Rp+t$, then $\\frac{\\partial q}{\\partial p}=R$ (you may use a symbolic math software to compute the Jacobians as well). In the following we use ${^1p} = \\begin{bmatrix} p_x & p_y & p_z \\end{bmatrix}^\\top$ and ${^2p} = \\begin{bmatrix} q_x & q_y & q_z \\end{bmatrix}^\\top$.\n",
    "$$H_1 = \\frac{\\partial h_1}{\\partial ~{^1p}} = K_f \\frac{\\partial \\pi}{\\partial ~{^1p}} = K_f \\begin{bmatrix}\n",
    "    \\frac{1}{p_z} & 0 & -\\frac{p_x}{p_z^2}\\\\ \n",
    "    0 & \\frac{1}{p_z} & -\\frac{p_y}{p_z^2}\n",
    "    \\end{bmatrix},$$\n",
    "$$H_2 = \\frac{\\partial h_2}{\\partial ~{^1p}} = K_f \\frac{\\partial \\pi}{\\partial ~{^1p}} = K_f \\frac{\\partial \\pi}{\\partial ~{^2p}} \\cdot \\frac{\\partial ~{^2p}}{\\partial ~{^1p}} = K_f \\begin{bmatrix}\n",
    "    \\frac{1}{q_z} & 0 & -\\frac{q_x}{q_z^2}\\\\ \n",
    "    0 & \\frac{1}{q_z} & -\\frac{q_y}{q_z^2}\n",
    "    \\end{bmatrix} R^\\top ,$$\n",
    "and the stacked measurement Jacobian can also be constrcuted as \n",
    "$$ H_{4\\times 3} = \\begin{bmatrix} H_1 \\\\ H_2 \\end{bmatrix}.$$\n",
    "\n",
    "#### Provided Data Format\n",
    "The processed measurements are provided in .mat and .csv files. Both files contain the same data. Unfortunately, there is no knowledge about the noise covariances, and tuning these parameters must be done manually as part of your implementation.\n",
    "\n",
    "In the provided data, \n",
    "- Kf\\_1 and Kf\\_2 are the intrinsic camera matrices for two cameras.\n",
    "- C\\_1 and C\\_2 are the coordinates of the optical center of the image for two cameras.\n",
    "- z\\_1 and z\\_2 are the 20 observations in the form of (u, v) for two cameras.\n",
    "- R and t are the rotation matrix and the translation vector of camera 2 with respect to camera 1.\n",
    "\n",
    "\n",
    "**Remark:** Generally speaking, the batch update is expected to be more accurate than the sequential update, especially for EKF. This is because in a sequential update, after each correction, the linearization point is changed. The Jacobian of the next correction is not evaluated at the same point as the previous correction. This process might cause unexpected outcomes, depending on the problem. In the batch case, all information is incorporated into the filter at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d8cd8",
   "metadata": {},
   "source": [
    "#### 1.a EKF (30 points)\n",
    "Develop an EKF to solve the problem using\n",
    "- A sequential measurement update that applies camera 1's correction first followed by camera 2's correction.\n",
    "- A batch measurement update using the stacked measurement model.\n",
    "\n",
    "#### Submission\n",
    "Please fill the **TODO**s in the function contained in **nonlinear_filtering_ekf.py** and submit the file to gradescope.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Derive and implement `process_model, measurement_model, measurement_Jacobain` functions (6 points)\n",
    "2. Implement `prediction, correction1, correction2` methods of `extended_kalman_filter` class. (6 points)\n",
    "3. Implement `ekf_sequential` function. (6 points)\n",
    "4. Implement `correction_batch` method of `extended_kalman_filter` class. (6 points)\n",
    "5. Implement `ekf_batch` function. (6 points)\n",
    "\n",
    "Your code is evaluated at each step using test input independently. Specifically, during steps 3 and 5, the autograder will use the correct `extended_kalman_filter` class to test your code. For example, even if the `prediction` method from step 2 is incorrect, implementing `ekf_sequential` correctly in step 3 can still earn you full credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonlinear_filtering_ekf import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a3dcc",
   "metadata": {},
   "source": [
    "#### EKF with sequential updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ekf_load_data()\n",
    "ekf = extended_kalman_filter(data)\n",
    "states = ekf_sequential(ekf, data).squeeze()\n",
    "\n",
    "# print final state\n",
    "print('Answer for Problem 1a (sequential):')\n",
    "print('Final x: %.4f, y: %.4f, z: %.4f' % (states[-1,0], states[-1,1], states[-1,2]))\n",
    "# plotting\n",
    "fig = plt.figure()\n",
    "plt.plot(states[:, 0], label='px')\n",
    "plt.plot(states[:, 1], label='py')\n",
    "plt.plot(states[:, 2], label ='pz')\n",
    "plt.xlabel(r'Time Step')\n",
    "plt.ylabel(r'Position')\n",
    "plt.legend()\n",
    "plt.title('EKF Sequential')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c3a3c",
   "metadata": {},
   "source": [
    "#### EKF with batch updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ff4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ekf_load_data()\n",
    "ekf = extended_kalman_filter(data)\n",
    "states = ekf_batch(ekf, data).squeeze()\n",
    "\n",
    "# print final state\n",
    "print('Answer for Problem 1a (batch):')\n",
    "print('Final x: %.4f, y: %.4f, z: %.4f' % (states[-1,0], states[-1,1], states[-1,2]))\n",
    "# plotting\n",
    "fig = plt.figure()\n",
    "plt.plot(states[:, 0], label='px')\n",
    "plt.plot(states[:, 1], label='py')\n",
    "plt.plot(states[:, 2], label ='pz')\n",
    "plt.xlabel(r'time Step')\n",
    "plt.ylabel(r'Position')\n",
    "plt.legend()\n",
    "plt.title('EKF Batch Measurement')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f8a1d",
   "metadata": {},
   "source": [
    "#### 1.b Particle Filter (30 points)\n",
    "Develop a particle filter to solve the problem using\n",
    "- A sequential measurement update that applies camera 1's correction first followed by camera 2's correction.\n",
    "- A batch measurement update using the stacked measurement model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a1835",
   "metadata": {},
   "source": [
    "#### Submission\n",
    "Please fill the **TODO**s in the function contained in **nonlinear_filtering_pf.py** and submit the file to gradescope.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Derive and implement `process_model, measurement_model` functions (6 points)\n",
    "2. Implement `sample_motion, importance_measurement_1, importance_measurement_2` methods of `particle_filter` class. (6 points)\n",
    "3. Implement `pf_sequential` function. (6 points)\n",
    "4. Implement `importance_mesurement_batch` method of `particle_filter` class. (6 points)\n",
    "5. Implement `pf_batch` function. (6 points)\n",
    "\n",
    "Your code is evaluated at each step using test input independently. Specifically, during steps 3 and 5, the autograder will use the correct `particle_filter` class to test your code. For example, even if the `sample_motion` method from step 2 is incorrect, implementing `pf_sequential` correctly in step 3 can still earn you full credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonlinear_filtering_pf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36c542",
   "metadata": {},
   "source": [
    "#### PF with sequential updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "np.random.seed(seed=42)\n",
    "data = pf_load_data()\n",
    "pf = particle_filter(data)\n",
    "states = pf_sequential(pf, data)\n",
    "\n",
    "# print final state\n",
    "print('Answer for Problem 1b (sequential):')\n",
    "print('Final x: %.4f, y: %.4f, z: %.4f' % (states[-1, 0], states[-1, 1], states[-1, 2]))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(states[:, 0], label='px')\n",
    "plt.plot(states[:, 1], label='py')\n",
    "plt.plot(states[:, 2], label='pz')\n",
    "plt.title('Particle Filter Sequential')\n",
    "plt.xlabel(r'time step')\n",
    "plt.ylabel(r'Position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7e305",
   "metadata": {},
   "source": [
    "#### PF with batch updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60094575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "np.random.seed(seed=42)\n",
    "data = pf_load_data()\n",
    "pf = particle_filter(data)\n",
    "states = pf_batch(pf, data)\n",
    "\n",
    "# print final state\n",
    "print('Answer for Problem 1b (batch):')\n",
    "print('Final x: %.4f, y: %.4f, z: %.4f' % (states[-1, 0], states[-1, 1], states[-1, 2]))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(states[:, 0], label='px')\n",
    "plt.plot(states[:, 1], label='py')\n",
    "plt.plot(states[:, 2], label='pz')\n",
    "plt.title('Particle Filter Batch ')\n",
    "plt.xlabel(r'time Step')\n",
    "plt.ylabel(r'Position')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69d9b6",
   "metadata": {},
   "source": [
    "### Problem 2. Attitude and Heading Reference System (AHRS) (40 points + 20 points bonus)\n",
    "\n",
    "In this problem, you will implement a Right-Invariant EKF (RI-EKF) on $\\mathrm{SO}(3)$ to estimate the sensor frame orientation wrt the world frame. An Inertial Measurement Unit (IMU), shown below, is a highly useful sensor that consists of a gyroscope, an accelerometer, and often a magnetometer as well as a barometer (to measure the pressure). The gyroscope measures the angular velocity in the body/sensor frame. The accelerometer measures linear acceleration in the body frame. The magnetometer measures the local magnetic field, providing an absolute reference for the heading angle. An AHRS is an onboard system that consists of an IMU and often an EKF to estimate the sensor orientation by fusing the mentioned measurements. \n",
    "\n",
    "![IMU](VN100.jpg)\n",
    "\n",
    "In this problem, we only recorded the raw gyroscope and accelerometer measurements from a real VN-100 IMU. We also recorded the estimated orientation provided by the embedded EKF developed by the manufacturer. Since the estimated orientation is highly optimized for this sensor and is accurate (uses all onboard sensors and factory calibration/filtering of the signals), we use it as a proxy for the ground truth. For more information about the sensor, refer to its user manual and datasheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b7530",
   "metadata": {},
   "source": [
    "The **process model** of the RI-EKF will integrate gyroscope measurements. Let $R_t \\in \\mathrm{SO}(3)$ be the state variable that shows the sensor frame orientation wrt the world frame. The dynamics is\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{R}_t &= R_t (\\tilde{\\omega}_t - w_t^g)^\\wedge ,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\tilde{\\omega}_t$ is the gyroscope reading and $w_t^g$ is the gyroscope noise modeled as a zero-mean white Gaussian process. \n",
    "The **correction model** uses the accelerometer as follows.\n",
    "\\begin{equation*} \n",
    "Y_{t_k} = R_{t_k}^{\\top} g + V_{t_k},\n",
    "\\end{equation*}\n",
    "where $g$ is the constant gravity vector (used as a reference) and $V_{t_k}$ is a vector of Gaussian noise. Here $Y_{t_k} = a_{t_k}$, where $a_{t_k}$ is the provided linear acceleration measurement. This observation model is clearly right-invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633c305",
   "metadata": {},
   "source": [
    "#### Provided Data Format\n",
    "\n",
    "The raw gyroscope and accelerometer measurements and ground truth orientation are provided in .mat and .csv files. Both files contain the same data. In the provided data, \n",
    "- $\\texttt{a}$ is the accelerometer readings.\n",
    "- $\\texttt{omega}$ is the gyroscope readings.\n",
    "- $\\texttt{dt}$ is the time difference (delta time) at each step in seconds.\n",
    "- $\\texttt{g}$ is the gravity vector.\n",
    "- $\\texttt{euler\\_gt}$ is the ground truth orientation as ZYX Euler angles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aebc92",
   "metadata": {},
   "source": [
    "#### 2.a RI-EKF (40 points)\n",
    "Implement the RI-EKF and run the filter using provided IMU data. \n",
    "\n",
    "#### Submission\n",
    "Please fill the **TODO**s in the function contained in **ahrs_riekf.py** and submit the file to gradescope.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Derive and implement the `motion_model` function (8 points)\n",
    "2. Derive and implement the `measurement_Jacobain` function (8 points)\n",
    "3. Implement the `prediction` method in the `right_iekf` class. (8 points)\n",
    "4. Implement the `correction` method in the `right_iekf` class. (8 points)\n",
    "5. Implement the `ahrs_riekf` funtion. (8 points)\n",
    "\n",
    "Your code is evaluated at each step using test input independently. Specifically, during steps 3 and 4, the autograder will use the correct `motion_model` and `measurement_Jacobain` to test your methods; during step 5, the autograder will use the correct `right_iekf` class to test your funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ahrs_riekf import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = riekf_load_data()\n",
    "iekf_filter = right_iekf()\n",
    "states_euler = ahrs_riekf(iekf_filter, data)\n",
    "\n",
    "dt = data['dt']\n",
    "euler_gt = data['euler_gt']\n",
    "\n",
    "# print final state\n",
    "print('Answer for Problem 2a:')\n",
    "print('Final x: %.4f, y: %.4f, z: %.4f' % (states_euler[-1,0], states_euler[-1,1], states_euler[-1,2]))\n",
    "\n",
    "# plotting\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "time = np.concatenate(([0],np.cumsum(dt)))\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "\n",
    "ax1.plot(time, states_euler[:, 0], label=\"z\", color='orange')\n",
    "ax1.plot(time, euler_gt[:, 0], label=\"z_gt\", color='darkgoldenrod')\n",
    "ax2.plot(time, states_euler[:, 1], label=\"y\", color='lightsalmon')\n",
    "ax2.plot(time, euler_gt[:, 1], label=\"y_gt\", color='orangered')\n",
    "ax3.plot(time, states_euler[:, 2], label=\"x\", color='slateblue')\n",
    "ax3.plot(time, euler_gt[:, 2], label=\"x_gt\", color='dodgerblue')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "\n",
    "ax.set_xlabel('time step')\n",
    "ax.set_ylabel('euler angles')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60497ae3",
   "metadata": {},
   "source": [
    "#### 2.b Imperfect RI-EKF (20 points bonus)\n",
    "Unfortunately, the sensor and motion models are not perfect. The accelerometer measurement is corrupted with bias. Therefore, we may incorporate a bias state. A simple way is to model the bias as being driven only by zero-mean white Gaussian noise. Thus, we have the new extended state model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{R}_t &= R_t (\\tilde{\\omega}_t - w_t^g)^\\wedge , \\\\\n",
    "\\dot{b}_t &= w_t^{b},\n",
    "\\end{align*}\n",
    "\n",
    "where $b_t \\in \\mathbb{R}^3$ is the accelerometer bias and $w_t^{b}$ is a zero-mean white Gaussian noise. We have the observation model as follows.\n",
    "\n",
    "\\begin{equation*} \n",
    "Y_{t_k} = R_{t_k}^{\\top} g + b_t + V_{t_k}.\n",
    "\\end{equation*}\n",
    "\n",
    "Note that due to the extended state $b_t$, the observation model is no longer right invariant. Thus, we may call it the imperfect RI-EKF.\n",
    "\n",
    "Derive all the equations for the implementation of the imperfect RI-EKF and implement the filter.\n",
    "\n",
    "**Hints:** Concatenate the linearized $R_t$ dynamics on $SO(3)$ with $b_t$ dynamics in $\\mathbb{R}^3$ for the prediction step. The new observation model is not right invariant due to the bias; derive the update steps as conventional EKF with residual defined in vector space. You may refer to Section 7 of this paper: \n",
    "- [Hartley R, Ghaffari M, Eustice RM, Grizzle JW. Contact-aided invariant extended Kalman filtering for robot state estimation. The International Journal of Robotics Research. 2020 Mar; 39(4):402-30.](https://journals.sagepub.com/doi/pdf/10.1177/0278364919894385)\n",
    "\n",
    "#### Submission\n",
    "Please fill the **TODO**s in the function contained in **ahrs_imperfect_riekf.py** and submit the file to gradescope.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Derive and implement the `motion_model` function (4 points)\n",
    "2. Derive and implement the `measurement_Jacobain` function (4 points)\n",
    "3. Implement the `prediction` method in the `imperfect_right_iekf` class. (4 points)\n",
    "4. Implement the `correction` method in the `imperfect_right_iekf` class. (4 points)\n",
    "5. Implement the `ahrs_imperfect_riekf` funtion. (4 points)\n",
    "\n",
    "Your code is evaluated at each step using test input independently. Specifically, during steps 3 and 4, the autograder will use the correct `motion_model` and `measurement_Jacobain` to test your methods; during step 5, the autograder will use the correct `imperfect_right_iekf` class to test your funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6757bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ahrs_imperfect_riekf import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = imperfect_riekf_load_data()\n",
    "inekf_filter = imperfect_right_iekf()\n",
    "states_euler, bias = ahrs_imperfect_riekf(inekf_filter, data)\n",
    "euler_gt = data['euler_gt']\n",
    "dt = data['dt']\n",
    "\n",
    "# print final state\n",
    "print('Answer for Problem 2b:')\n",
    "print('Final x: %.4f, y: %.4f, z: %.4f' % (states_euler[-1,0], states_euler[-1,1], states_euler[-1,2]))\n",
    "\n",
    "# plot euler angles\n",
    "fig = plt.figure()\n",
    "time = np.concatenate(([0],np.cumsum(dt)))\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "\n",
    "ax1.plot(time, states_euler[:, 0], label=\"z\", color='orange')\n",
    "ax1.plot(time, euler_gt[:, 0], label=\"z_gt\", color='darkgoldenrod')\n",
    "ax2.plot(time, states_euler[:, 1], label=\"y\", color='lightsalmon')\n",
    "ax2.plot(time, euler_gt[:, 1], label=\"y_gt\", color='orangered')\n",
    "ax3.plot(time, states_euler[:, 2], label=\"x\", color='slateblue')\n",
    "ax3.plot(time, euler_gt[:, 2], label=\"x_gt\", color='dodgerblue')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "\n",
    "ax.set_xlabel('time step')\n",
    "ax.set_ylabel('euler angles')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot bias\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "\n",
    "ax1.plot(time, bias[:, 0], label=\"z\", color='orange', )\n",
    "ax2.plot(time, bias[:, 1], label=\"y\", color='lightsalmon')\n",
    "ax3.plot(time, bias[:, 2], label=\"x\", color='slateblue')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "\n",
    "ax.set_xlabel('time step')\n",
    "ax.set_ylabel('bias')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
